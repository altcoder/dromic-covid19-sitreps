import airflow
import boto3
import json
import logging
from airflow import DAG
from datetime import timedelta

from airflow.contrib.hooks.snowflake_hook import SnowflakeHook
from airflow.utils.dates import days_ago
from airflow.operators.python_operator import PythonOperator
from airflow.contrib.operators.snowflake_operator import SnowflakeOperator
from airflow.operators.dummy_operator import DummyOperator
from airflow.models import Variable

import os
import glob
import papermill as pm
from airflow.configuration import conf
import tempfile

import gspread
from gspread.exceptions import WorksheetNotFound
from oauth2client.service_account import ServiceAccountCredentials

DAGS_FOLDER = conf.get('core', 'dags_folder')
NOTEBOOKS_FOLDER = os.path.abspath(
    conf.get('core', 'dags_folder') + "/../notebooks") + "/"
OUTPUT_FOLDER = os.path.abspath(
    conf.get('core', 'dags_folder') + "/../output") + "/"

with open( DAGS_FOLDER + "/../config/refresh_schedules.json", 'r') as f:
    schedules = json.load(f)

def create_dag(dag_id, args):

    # notebook name without extension, for example "JHU_COVID-19"
    basename = args.get('basename')

    # notebook file, this will be executed, for example: /home/ec2-user/COVID-19-data/notebooks/JHU_COVID-19.ipynb
    notebook_file = NOTEBOOKS_FOLDER + basename + ".ipynb"

    # directory to look for output files
    output_root = OUTPUT_FOLDER

    # the csv file location which wil be generated by the notebook, for example: /home/ec2-user/output/dromic-covid19-sitreps_2020-04-04.csv
    output_file_glob = OUTPUT_FOLDER + basename + '*'

    dag = DAG(
        dag_id=dag_id,
        default_args=args,
        max_active_runs=1,
        schedule_interval=schedules["interval"].get(basename,None),
        dagrun_timeout=timedelta(minutes=60)
    )


    with dag:
        start = DummyOperator(
            task_id='start',
            dag=dag
        )

        def clean_generated_files():
            for output_file in glob.glob(output_file_glob):
                if os.path.exists(output_file):
                    os.remove(output_file)

        def execute_notebook():
            pm.execute_notebook(
                input_path=notebook_file,
                output_path="/dev/null",
                parameters=dict(
                    {"output_folder": OUTPUT_FOLDER}),
                log_output=True,
                report_mode=True
            )
            return

        def upload_to_gsheet():
            scopes = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
            # Change this based on your google api credentials.json file
            keyfile_dict = {
                "type": "service_account",
                "project_id": Variable.get('GSHEET_PROJECT_ID'),
                "private_key_id": Variable.get('GSHEET_PRIVATE_KEY_ID'),
                "private_key": Variable.get('GSHEET_PRIVATE_KEY'),
                "client_email": Variable.get('GSHEET_CLIENT_EMAIL'),
                "client_id":  Variable.get('GSHEET_CLIENT_ID'),
                "auth_uri": "https://accounts.google.com/o/oauth2/auth",
                "token_uri": "https://oauth2.googleapis.com/token",
                "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
                "client_x509_cert_url": Variable.get('GSHEET_CERT_URL'),
            }
            credentials = ServiceAccountCredentials.from_json_keyfile_dict(keyfile_dict, scopes=scopes)
            gc = gspread.authorize(credentials)
            sheet = gc.open_by_key(Variable.get('GSHEET_SPREADSHEET_ID'))
            sorted_files =list(glob.glob(output_file_glob + ".csv"))
            sorted_files.sort()
            for output_file in sorted_files:
                logging.info('Processing %s' % output_file)
                file_name = os.path.basename(output_file)
                tab_name = os.path.splitext(file_name)[0].replace('-', '_').replace(basename+'_', '')
                try:
                    wks = sheet.worksheet(tab_name)
                except WorksheetNotFound:
                    logging.info('Creating NEW worksheet "%s"' % tab_name)
                    #sheet.del_worksheet(wks)
                    sheet.add_worksheet(title=tab_name, rows='100', cols='20')
                    wks = sheet.worksheet(tab_name)
                with open(output_root + file_name, 'r') as csv:
                    csv_contents = csv.read()
                body = {
                    'requests': [{
                        'pasteData': {
                            "coordinate": {
                                "sheetId": wks.id,
                                "rowIndex": 0,
                                "columnIndex": 0,
                            },
                            "data": csv_contents,
                            "type": 'PASTE_NORMAL',
                            "delimiter": ',',
                        }
                    }]
                }
                response = sheet.batch_update(body)
            return response

        def upload_to_s3():
            """Upload a file to an S3 bucket

            :param basename
            :return: True if file was uploaded, else False
            """
            response = False

            s3_client = boto3.client('s3', aws_access_key_id=Variable.get("AWS_ACCESS_KEY_ID"),
                                     aws_secret_access_key=Variable.get("AWS_SECRET_ACCESS_KEY"))
            s3_path = Variable.get("AWS_S3_PATH")
            for output_file in glob.glob(output_file_glob):
                logging.info('Processing %s' % output_file)
                file_name = os.path.basename(output_file)
                s3_file_name = file_name.replace('-', '_').replace(basename+'_', '')
                s3_table_name = basename
                response = s3_client.upload_file(output_file,
                                                 Variable.get("AWS_S3_BUCKET"),
                                                 s3_path + '/' + s3_table_name + '/' + s3_file_name)

            return response

        def upload_to_snowflake(task_id):
            sql_statements = []
            snowflake_schema = Variable.get("SNOWFLAKE_SCHEMA", default_var="PUBLIC")
            sql_statements.append(f'USE SCHEMA {snowflake_schema}')

            snowflake_table_name = basename
            truncate_st = f'TRUNCATE TABLE IF EXISTS {snowflake_table_name}'
            sql_statements.append(truncate_st)

            # look for CSVs to ignest to snowflake
            snowflake_stage = Variable.get("SNOWFLAKE_STAGE", default_var="RAW")
            snowflake_path = Variable.get('SNOWFLAKE_PATH', default_var='prod/dswd')

            insert_st = f'copy into {snowflake_table_name} from @{snowflake_stage}/{snowflake_path}/{snowflake_table_name}/ file_format = (type = "csv" field_delimiter = "," NULL_IF = (\'NULL\', \'null\',\'\') EMPTY_FIELD_AS_NULL = true FIELD_OPTIONALLY_ENCLOSED_BY=\'"\' skip_header = 1) pattern=\'.*\\.csv\''
            sql_statements.append(insert_st)
            sql_statements.append("COMMIT")

            create_insert_task = SnowflakeOperator(
                task_id=task_id,
                sql=sql_statements,
                autocommit=False,
                snowflake_conn_id=Variable.get("SNOWFLAKE_CONNECTION", default_var="SNOWFLAKE_PROD"),
            )

            return create_insert_task

        def create_dynamic_etl(task_id, callable_function):
            task = PythonOperator(
                task_id=task_id,
                python_callable=callable_function,
                dag=dag,
            )
            return task

        end = DummyOperator(
            task_id='end',
            dag=dag)

        cleanup_output_folder_task = create_dynamic_etl(
            'cleanup', clean_generated_files)

        execute_notebook_task = create_dynamic_etl(
            'execute_notebook', execute_notebook)

        upload_to_gsheet_task = create_dynamic_etl('upload_to_gsheet', upload_to_gsheet)

        upload_to_s3_task = create_dynamic_etl('upload_to_s3', upload_to_s3)

        upload_to_snowflake_task = upload_to_snowflake('upload_to_snowflake')

        #start >> cleanup_output_folder_task
        #cleanup_output_folder_task >> execute_notebook_task
        start >> execute_notebook_task
        execute_notebook_task >> upload_to_gsheet_task
        upload_to_gsheet_task >> upload_to_s3_task
        upload_to_s3_task >> upload_to_snowflake_task
        upload_to_snowflake_task >> end

        return dag


#  Look for python notebooks to execute
for file in os.listdir(NOTEBOOKS_FOLDER):
    if file.startswith("."):
        continue
    filename_without_extension = os.path.splitext(file)[0]
    dag_id = 'etl_{}'.format(str(filename_without_extension))

    default_args = {'owner': 'admin',
                    'start_date': days_ago(2),
                    'basename': filename_without_extension
                    }
    globals()[dag_id] = create_dag(dag_id, default_args)
